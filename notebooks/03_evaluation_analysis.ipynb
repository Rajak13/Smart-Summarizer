{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c688166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ rouge library not found. Installing rouge-score...\n",
      "✓ Successfully installed rouge-score\n",
      "✗ Installation succeeded but import still fails.\n",
      "  Please restart the kernel and run this cell again.\n"
     ]
    }
   ],
   "source": [
    "# FIX: Install and verify rouge-score package\n",
    "# Run this cell FIRST if you get \"ModuleNotFoundError: No module named 'rouge'\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Install package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--quiet\"])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "# Check if rouge is available\n",
    "try:\n",
    "    from rouge import Rouge\n",
    "    print(\"✓ rouge library is already installed\")\n",
    "except ImportError:\n",
    "    print(\"⚠ rouge library not found. Installing rouge-score...\")\n",
    "    if install_package(\"rouge-score\"):\n",
    "        print(\"✓ Successfully installed rouge-score\")\n",
    "        # Try importing again\n",
    "        try:\n",
    "            from rouge import Rouge\n",
    "            print(\"✓ rouge library now available\")\n",
    "        except ImportError:\n",
    "            print(\"✗ Installation succeeded but import still fails.\")\n",
    "            print(\"  Please restart the kernel and run this cell again.\")\n",
    "    else:\n",
    "        print(\"✗ Failed to install rouge-score\")\n",
    "        print(\"  Please run manually: pip install rouge-score\")\n",
    "        print(\"  Then restart the kernel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa43993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Import error: No module named 'rouge'\n",
      "  Make sure you've run the previous cell to install dependencies\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbart\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BARTSummarizer\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpegasus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PEGASUSSummarizer\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SummarizerEvaluator\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ All imports successful\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/smart-summarizer/notebooks/../utils/evaluator.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mComprehensive Evaluation System for Summarization Models\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mImplements ROUGE metrics, comparison analysis, and statistical testing\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrouge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Rouge\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Tuple, Optional\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'rouge'"
     ]
    }
   ],
   "source": [
    "# Add project root to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import models and utilities\n",
    "try:\n",
    "    from models.textrank import TextRankSummarizer\n",
    "    from models.bart import BARTSummarizer\n",
    "    from models.pegasus import PEGASUSSummarizer\n",
    "    from utils.evaluator import SummarizerEvaluator\n",
    "    from utils.data_loader import DataLoader\n",
    "    print(\"✓ All imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import error: {e}\")\n",
    "    print(\"  Make sure you've run the previous cell to install dependencies\")\n",
    "    raise\n",
    "\n",
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28695c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading test dataset...\")\n",
    "loader = DataLoader()\n",
    "\n",
    "# Load your saved samples (or load fresh)\n",
    "try:\n",
    "    test_data = loader.load_samples('../data/samples/test_50.json')\n",
    "    print(f\"✓ Loaded {len(test_data)} test samples\")\n",
    "except:\n",
    "    print(\"Downloading test data...\")\n",
    "    test_data = loader.load_cnn_dailymail(split='test', num_samples=50)\n",
    "    loader.save_samples(test_data, '../data/samples/test_50.json')\n",
    "    print(f\"✓ Downloaded and saved {len(test_data)} samples\")\n",
    "\n",
    "# Extract texts and references\n",
    "texts = [item['article'] for item in test_data]\n",
    "references = [item['reference_summary'] for item in test_data]\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  - Number of samples: {len(texts)}\")\n",
    "print(f\"  - Avg article length: {np.mean([len(t.split()) for t in texts]):.0f} words\")\n",
    "print(f\"  - Avg reference length: {np.mean([len(r.split()) for r in references]):.0f}words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7dc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInitializing models...\")\n",
    "\n",
    "models = {\n",
    "    'TextRank': TextRankSummarizer(),\n",
    "    'BART': BARTSummarizer(device='cpu'),\n",
    "    'PEGASUS': PEGASUSSummarizer(device='cpu')\n",
    "}\n",
    "\n",
    "print(\"✓ All models ready\")\n",
    "\n",
    "# Cell 4: Generate Summaries (Takes ~10-20 minutes for 50 samples)\n",
    "print(\"\\nGenerating summaries for all models...\")\n",
    "print(\"This will take 10-20 minutes. Grab a coffee! ☕\")\n",
    "\n",
    "all_summaries = {}\n",
    "all_times = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    summaries = []\n",
    "    times = []\n",
    "    \n",
    "    for i, text in enumerate(texts[:10], 1):  # Start with 10 samples\n",
    "        print(f\"  Processing {i}/10...\", end='\\r')\n",
    "        \n",
    "        if model_name == 'TextRank':\n",
    "            result = model.summarize_with_metrics(text)\n",
    "        else:\n",
    "            result = model.summarize_with_metrics(text, max_length=100, min_length=30)\n",
    "        \n",
    "        summaries.append(result['summary'])\n",
    "        times.append(result['metadata']['processing_time'])\n",
    "    \n",
    "    all_summaries[model_name] = summaries\n",
    "    all_times[model_name] = times\n",
    "    print(f\"  ✓ Completed {model_name}                    \")\n",
    "\n",
    "print(\"\\n✓ All summaries generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating models...\")\n",
    "\n",
    "evaluator = SummarizerEvaluator()\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name in models.keys():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    results = evaluator.evaluate_batch(\n",
    "        all_summaries[model_name],\n",
    "        references[:len(all_summaries[model_name])],\n",
    "        model_name\n",
    "    )\n",
    "    results['avg_time'] = np.mean(all_times[model_name])\n",
    "    results['std_time'] = np.std(all_times[model_name])\n",
    "    evaluation_results[model_name] = results\n",
    "\n",
    "print(\"✓ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ebcf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    results_table.append({\n",
    "        'Model': model_name,\n",
    "        'Type': 'Extractive' if model_name == 'TextRank' else 'Abstractive',\n",
    "        'ROUGE-1': f\"{results['rouge_1_f1_mean']:.4f} ± {results['rouge_1_f1_std']:.4f}\",\n",
    "        'ROUGE-2': f\"{results['rouge_2_f1_mean']:.4f} ± {results['rouge_2_f1_std']:.4f}\",\n",
    "        'ROUGE-L': f\"{results['rouge_l_f1_mean']:.4f} ± {results['rouge_l_f1_std']:.4f}\",\n",
    "        'Avg Time (s)': f\"{results['avg_time']:.3f} ± {results['std_time']:.3f}\",\n",
    "        'Samples': results['num_samples']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_table)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV for report\n",
    "results_df.to_csv('../results/evaluation_results.csv', index=False)\n",
    "print(\"\\n✓ Results saved to results/evaluation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65fac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare BART vs PEGASUS (both abstractive)\n",
    "bart_rouge1 = [s['rouge_1_f1'] for s in evaluation_results['BART']['individual_scores']]\n",
    "peg_rouge1 = [s['rouge_1_f1'] for s in evaluation_results['PEGASUS']['individual_scores']]\n",
    "\n",
    "sig_test = evaluator.statistical_significance_test(\n",
    "    bart_rouge1,\n",
    "    peg_rouge1,\n",
    "    test_name='paired t-test'\n",
    ")\n",
    "\n",
    "print(f\"\\nBART vs PEGASUS (ROUGE-1):\")\n",
    "print(f\"  Test: {sig_test['test_name']}\")\n",
    "print(f\"  p-value: {sig_test['p_value']:.6f}\")\n",
    "print(f\"  {sig_test['interpretation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae272f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Create grid\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. ROUGE Scores Comparison\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "rouge_data = pd.DataFrame({\n",
    "    'Model': list(evaluation_results.keys()) * 3,\n",
    "    'Metric': ['ROUGE-1']*3 + ['ROUGE-2']*3 + ['ROUGE-L']*3,\n",
    "    'Score': [\n",
    "        evaluation_results['TextRank']['rouge_1_f1_mean'],\n",
    "        evaluation_results['BART']['rouge_1_f1_mean'],\n",
    "        evaluation_results['PEGASUS']['rouge_1_f1_mean'],\n",
    "        evaluation_results['TextRank']['rouge_2_f1_mean'],\n",
    "        evaluation_results['BART']['rouge_2_f1_mean'],\n",
    "        evaluation_results['PEGASUS']['rouge_2_f1_mean'],\n",
    "        evaluation_results['TextRank']['rouge_l_f1_mean'],\n",
    "        evaluation_results['BART']['rouge_l_f1_mean'],\n",
    "        evaluation_results['PEGASUS']['rouge_l_f1_mean']\n",
    "    ]\n",
    "})\n",
    "\n",
    "sns.barplot(data=rouge_data, x='Metric', y='Score', hue='Model', ax=ax1)\n",
    "ax1.set_title('ROUGE Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('F1 Score')\n",
    "ax1.set_ylim([0, 0.5])\n",
    "ax1.legend(title='Model')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Processing Time\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "times = [evaluation_results[m]['avg_time'] for m in models.keys()]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "ax2.bar(models.keys(), times, color=colors)\n",
    "ax2.set_title('Processing Time', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. ROUGE-1 Distribution\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "for model_name, color in zip(models.keys(), colors):\n",
    "    rouge1_scores = [s['rouge_1_f1'] for s in evaluation_results[model_name]['individual_scores']]\n",
    "    ax3.hist(rouge1_scores, alpha=0.6, label=model_name, bins=10, color=color)\n",
    "ax3.set_title('ROUGE-1 Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('ROUGE-1 F1 Score')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. ROUGE-2 Distribution\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "for model_name, color in zip(models.keys(), colors):\n",
    "    rouge2_scores = [s['rouge_2_f1'] for s in evaluation_results[model_name]['individual_scores']]\n",
    "    ax4.hist(rouge2_scores, alpha=0.6, label=model_name, bins=10, color=color)\n",
    "ax4.set_title('ROUGE-2 Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('ROUGE-2 F1 Score')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. ROUGE-L Distribution\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "for model_name, color in zip(models.keys(), colors):\n",
    "    rougel_scores = [s['rouge_l_f1'] for s in evaluation_results[model_name]['individual_scores']]\n",
    "    ax5.hist(rougel_scores, alpha=0.6, label=model_name, bins=10, color=color)\n",
    "ax5.set_title('ROUGE-L Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax5.set_xlabel('ROUGE-L F1 Score')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Box Plot Comparison\n",
    "ax6 = fig.add_subplot(gs[2, :])\n",
    "box_data = []\n",
    "for model_name in models.keys():\n",
    "    rouge1_scores = [s['rouge_1_f1'] for s in evaluation_results[model_name]['individual_scores']]\n",
    "    for score in rouge1_scores:\n",
    "        box_data.append({'Model': model_name, 'ROUGE-1': score})\n",
    "\n",
    "box_df = pd.DataFrame(box_data)\n",
    "sns.boxplot(data=box_df, x='Model', y='ROUGE-1', ax=ax6, palette=colors)\n",
    "ax6.set_title('ROUGE-1 Score Distribution (Box Plot)', fontsize=14, fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.savefig('../results/comprehensive_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Comprehensive visualization saved!\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING RESULTS FOR REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comprehensive export\n",
    "export_data = {\n",
    "    'evaluation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset': {\n",
    "        'name': 'CNN/DailyMail',\n",
    "        'samples_evaluated': len(all_summaries['TextRank']),\n",
    "        'split': 'test'\n",
    "    },\n",
    "    'models': {\n",
    "        model_name: {\n",
    "            'type': results_table[i]['Type'],\n",
    "            'rouge_1': {\n",
    "                'mean': evaluation_results[model_name]['rouge_1_f1_mean'],\n",
    "                'std': evaluation_results[model_name]['rouge_1_f1_std']\n",
    "            },\n",
    "            'rouge_2': {\n",
    "                'mean': evaluation_results[model_name]['rouge_2_f1_mean'],\n",
    "                'std': evaluation_results[model_name]['rouge_2_f1_std']\n",
    "            },\n",
    "            'rouge_l': {\n",
    "                'mean': evaluation_results[model_name]['rouge_l_f1_mean'],\n",
    "                'std': evaluation_results[model_name]['rouge_l_f1_std']\n",
    "            },\n",
    "            'processing_time': {\n",
    "                'mean': evaluation_results[model_name]['avg_time'],\n",
    "                'std': evaluation_results[model_name]['std_time']\n",
    "            }\n",
    "        }\n",
    "        for i, model_name in enumerate(models.keys())\n",
    "    },\n",
    "    'statistical_tests': {\n",
    "        'bart_vs_pegasus': sig_test\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../results/final_evaluation.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(\"✓ Exported to results/final_evaluation.json\")\n",
    "print(\"\\nFiles created for your report:\")\n",
    "print(\"  1. results/evaluation_results.csv - Table for report\")\n",
    "print(\"  2. results/comprehensive_evaluation.png - Main figure\")\n",
    "print(\"  3. results/final_evaluation.json - All data\")\n",
    "\n",
    "# Cell 10: Summary for Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS FOR YOUR REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_model = max(evaluation_results.keys(), \n",
    "                 key=lambda x: evaluation_results[x]['rouge_1_f1_mean'])\n",
    "fastest_model = min(evaluation_results.keys(),\n",
    "                    key=lambda x: evaluation_results[x]['avg_time'])\n",
    "\n",
    "print(f\"\\n1. Best Overall Performance: {best_model}\")\n",
    "print(f\"   - ROUGE-1: {evaluation_results[best_model]['rouge_1_f1_mean']:.4f}\")\n",
    "print(f\"   - ROUGE-2: {evaluation_results[best_model]['rouge_2_f1_mean']:.4f}\")\n",
    "print(f\"   - ROUGE-L: {evaluation_results[best_model]['rouge_l_f1_mean']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Fastest Processing: {fastest_model}\")\n",
    "print(f\"   - Avg time: {evaluation_results[fastest_model]['avg_time']:.3f}s\")\n",
    "print(f\"   - {evaluation_results[max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['avg_time'])]['avg_time'] / evaluation_results[fastest_model]['avg_time']:.1f}x faster than slowest\")\n",
    "\n",
    "print(f\"\\n3. Extractive vs Abstractive:\")\n",
    "print(f\"   - TextRank (Extractive): ROUGE-1 = {evaluation_results['TextRank']['rouge_1_f1_mean']:.4f}\")\n",
    "print(f\"   - BART (Abstractive): ROUGE-1 = {evaluation_results['BART']['rouge_1_f1_mean']:.4f}\")\n",
    "print(f\"   - PEGASUS (Abstractive): ROUGE-1 = {evaluation_results['PEGASUS']['rouge_1_f1_mean']:.4f}\")\n",
    "print(f\"   - Abstractive models outperform extractive by {(evaluation_results[best_model]['rouge_1_f1_mean'] / evaluation_results['TextRank']['rouge_1_f1_mean'] - 1) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ Evaluation complete! Use these results in your report.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
